# Random Artificial Intelligence & Machine Learning Notes  

There is an AI *theme* applied to virtually anything at work.  My experience says that signals simple *hype* but my reading suggests that this hum of *AI-everything* is ignored at my peril.  

This is alpha interest work.  After percolating in this format for a while, I'll figure out how to better prioritize this reading and content.  

## Where do we encounter AI/ML?  
* Games  
* Investing  
* Logistics systems  
* Medical diagnosis  
* autonomous driving  
* Language translation  
* Interactive personal assistance  
* *Consumer*/consumtion suggestions and advice (and product/service reviews)  


## Resources:  
* Allen Institute for Artificial Intelligence [https://allenai.org/](https://allenai.org/)  
* AI2 Leader Boards [https://leaderboard.allenai.org/](https://leaderboard.allenai.org/)  
* One view of *current topics* in AI/ML: [https://arxiv.org/search/<*truncated*>](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=cs.AI&terms-0-field=all&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first)  


# Natural Language Processing and Common Sense
### Types of Reasoning:  
* **Human intuition and instinct**: Its processes are unconscious, *effortless*, fast, associative, 'automatic-pilot,' slow-learning.  Its content is conceptual, temporal (for example before and after) and can be evoked by language. [D.Kahneman1](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
* **Rational thinking**: Its processes are require effort, and are slow, *logical*, governed by rules, often serial, sometimes flexible, and indecisive.  Its content is conceptual, temporal and can be evoked by language. [D.Kahneman1](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
* 
* 
* 

## Timeline Ideas:  
2021:  
2020:  
2019:  
2018:  
* 'Human-level performance' on reading comprehension (limited data set & limited definition of *comprehension*)  
2017:  
* 'Super-human performance' on speech recognition  
* Google’s [Alpha Go](https://en.wikipedia.org/wiki/AlphaGo) [defeats Chinese Go champion, Ke Jie](https://www.deepmind.com/research/highlighted-research/alphago)
2016:  
* 'Google neural' machine translation  
2015:  
* 'Super-human performance' on image captioning  
* 'Super-human performance' on object recognition  
2014:  
2013:  
2012:  
2011: IBM's Watson wins Jeopardy.  Apple introduces Siri.  
2010:  
1955: "[A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence. August 31, 1955."](https://ojs.aaai.org/index.php/aimagazine/article/download/1904/1802)  


#### Timeline Notes:  
* Stanford AI 100 [https://ai100.stanford.edu/reflections-and-framing](https://ai100.stanford.edu/reflections-and-framing)  
* "The History of Artificial Intelligence." by Rockwell Anyoha, 2017 [https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/](https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/)  
* "[The History of Artificial Intelligence.](https://courses.cs.washington.edu/courses/csep590/06au/projects/history-ai.pdf)" from 'History of Computing' CSEP 590A, University of Washington, December 2006, By Chris Smith, Brian McGuire, Ting Huang and Gary Yang.  

## Some Definitions:  

* *Common Sense* is the basic level of 'practical' knowledge and reasoning concerning everyday situations and events that are commonly shared among most people.  It is essential for humans to live and interact with each other in a reasonable and safe way. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) [at](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  *Common Sense* is a difficult challenge and integration into AI implementations still suffer from:  
  * Common sense inferences are about predicting new information that is likely to be true based on partially available information. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) at: [https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  
  * Common sense is essential for humans to live and interact with each other in a reasonable and safe way, and it is essential for AI to understand human needs and actions better. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) at: [https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  
  * "Insufficient coverage" --> Pre-trained language models contain *some* commonsense knowledge -- but an impractically small subset of what is needed for most use cases.  
  * "Insufficient precision" --> Language models also generate false facts -- which can be an annoyance or a catastrophy, depending on your use case.  
* *Machine learning* (ML) is often viewed as a subset of *Artificial Intelligence*.  ML employes previously collected data to predict outcomes.  ML *models* may depend upon direct human inputs (*training or supervision*), or not, depending on their algorithms.  ML enables software (*and supporting infrastructure*) to build upon training/experience and improvise suggestions or results.  
* *Natural language processing* (NLP) is a growing field within artificial intelligence. The fundamental goal of NLP is to program computers capable of human-level understanding of natural language. Common NLP applications include personal assistants and chatbots, automatic translation, question answering, sentiment analysis and summarization. Among the main challenges of NLP research is that human language is often ambiguous and underspecified. A person processing language relies heavily on their commonsense knowledge and reasoning abilities to resolve these ambiguities and complete missing information. Machine learning based NLP models, on the other hand, lack this commonsense and often make absurd mistakes. From [Dr. Vered Shwartz](https://www.cs.ubc.ca/~vshwartz/index.html) in her [2022 NLP course description](https://www.cs.ubc.ca/~vshwartz/courses/CPSC532V-22/index.html)  
* *Generative adversarial networks* (GANs) and *reinforcement learning* endow *deep networks* with the ability to produce artificial content such as fake images that pass for the real thing. "GANs consist of two interlocked components—a generator, responsible for creating realistic content, and a discriminator, tasked with distinguishing the output of the generator from naturally occurring content. The two learn from each other, becoming better and better at their respective tasks over time" [AI100Report_MT_10, page 12](https://ai100.stanford.edu/sites/g/files/sbiybj18871/files/media/file/AI100Report_MT_10.pdf).  
 

## Some References:  
* "Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report." September 2021 [http://ai100.stanford.edu/2021-report](http://ai100.stanford.edu/2021-report)  
* Kahneman, Daniel.  "Thinking, Fast and Slow." [Amazon](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
  
(https://en.wikipedia.org/wiki/Zero-shot_learning)  
ImageNet (zero-shot): SOTA, surpassing OpenAI CLIP (https://openai.com/blog/clip/).
LAMA (factual and commonsense knowledge): Surpassed AutoPrompt (https://arxiv.org/abs/2010.15980).
LAMBADA (cloze tasks): Surpassed Microsoft Turing NLG (https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/).
SuperGLUE (few-shot): SOTA, surpassing OpenAI GPT-3 (https://arxiv.org/abs/2005.14165).
UC Merced Land Use (zero-shot): SOTA, surpassing OpenAI CLIP (https://openai.com/blog/clip/).
MS COCO (text generation diagram): Surpassed OpenAI DALL·E (https://openai.com/blog/dall-e/).
MS COCO (English graphic retrieval): Surpassed OpenAI CLIP and Google ALIGN (https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html).
MS COCO (multilingual graphic retrieval): Surpassed UC² (best multilingual and multimodal pre-trained model) (https://arxiv.org/pdf/2104.00332.pdf).

