# Random Artificial Intelligence & Machine Learning Notes  

There is an AI *theme* applied to virtually anything at work.  My experience says that signals simple *hype* but my reading suggests that this hum of *AI-everything* is ignored at my peril.  

This is alpha interest work.  After percolating in this format for a while, I'll figure out how to better prioritize this reading and content.  


## Resources:  
Allen Institute for Artificial Intelligence [https://allenai.org/](https://allenai.org/)  
AI2 Leader Boards [https://leaderboard.allenai.org/](https://leaderboard.allenai.org/)  
One view of *current topics* in AI/ML: [https://arxiv.org/search/<*truncated*>](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=cs.AI&terms-0-field=all&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first)  


# Natural Language Processing and Common Sense
### Types of Reasoning:  
* **Human intuition and instinct**: Its processes are unconscious, *effortless*, fast, associative, 'automatic-pilot,' slow-learning.  Its content is conceptual, temporal (for example before and after) and can be evoked by language. [D.Kahneman1](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
* **Rational thinking**: Its processes are require effort, and are slow, *logical*, governed by rules, often serial, sometimes flexible, and indecisive.  Its content is conceptual, temporal and can be evoked by language. [D.Kahneman1](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
* 
* 
* 

## Timeline Ideas:  
2021:  
2020:  
2019:  
2018:  
* 'Human-level performance' on reading comprehension (limited data set)  
2017:  
* 'Super-human performance' on speech recognition  
2016:  
* 'Google neural' machine translation  
2015:  
* 'Super-human performance' on image captioning  
* 'Super-human performance' on object recognition  
2014:  
2013:  
2012:  
2011:  
2010:  


## Some Definitions:  

* *Common Sense* is the basic level of 'practical' knowledge and reasoning concerning everyday situations and events that are commonly shared among most people.  It is essential for humans to live and interact with each other in a reasonable and safe way. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) [at](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  *Common Sense* is a difficult challenge and integration into AI implementations still suffer from:  
  * Common sense inferences are about predicting new information that is likely to be true based on partially available information. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) at: [https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  
  * Common sense is essential for humans to live and interact with each other in a reasonable and safe way, and it is essential for AI to understand human needs and actions better. [Yejin Choi](https://homes.cs.washington.edu/~yejin/) at: [https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf](https://homes.cs.washington.edu/~msap/acl2020-commonsense/slides/01%20-%20Intro.pdf)  
  * "Insufficient coverage" --> Pre-trained language models contain *some* commonsense knowledge -- but an impractically small subset of what is needed for most use cases.  
  * "Insufficient precision" --> Language models also generate false facts -- which can be an annoyance or a catastrophy, depending on your use case.  
* *Natural language processing* (NLP) is a growing field within artificial intelligence. The fundamental goal of NLP is to program computers capable of human-level understanding of natural language. Common NLP applications include personal assistants and chatbots, automatic translation, question answering, sentiment analysis and summarization. Among the main challenges of NLP research is that human language is often ambiguous and underspecified. A person processing language relies heavily on their commonsense knowledge and reasoning abilities to resolve these ambiguities and complete missing information. Machine learning based NLP models, on the other hand, lack this commonsense and often make absurd mistakes. by [Dr. Vered Shwartz](https://www.cs.ubc.ca/~vshwartz/index.html) in her [2022 NLP course description](https://www.cs.ubc.ca/~vshwartz/courses/CPSC532V-22/index.html)  


## Some References:  
* Kahneman, Daniel.  "Thinking, Fast and Slow." [Amazon](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555/)  
  
(https://en.wikipedia.org/wiki/Zero-shot_learning)  
ImageNet (zero-shot): SOTA, surpassing OpenAI CLIP (https://openai.com/blog/clip/).
LAMA (factual and commonsense knowledge): Surpassed AutoPrompt (https://arxiv.org/abs/2010.15980).
LAMBADA (cloze tasks): Surpassed Microsoft Turing NLG (https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/).
SuperGLUE (few-shot): SOTA, surpassing OpenAI GPT-3 (https://arxiv.org/abs/2005.14165).
UC Merced Land Use (zero-shot): SOTA, surpassing OpenAI CLIP (https://openai.com/blog/clip/).
MS COCO (text generation diagram): Surpassed OpenAI DALL·E (https://openai.com/blog/dall-e/).
MS COCO (English graphic retrieval): Surpassed OpenAI CLIP and Google ALIGN (https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html).
MS COCO (multilingual graphic retrieval): Surpassed UC² (best multilingual and multimodal pre-trained model) (https://arxiv.org/pdf/2104.00332.pdf).

